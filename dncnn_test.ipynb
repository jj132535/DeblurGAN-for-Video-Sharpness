{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6LDIt96YP7YdciXlUOzmB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f4ab075bc8ee4743aa81373c391ad414": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cb365c36f8e24030b5ae24c7380579d6",
              "IPY_MODEL_b097b3b933b04da2b2a1adf0aeb09ac6",
              "IPY_MODEL_894bb09db33d471cb270ccd2487ce427"
            ],
            "layout": "IPY_MODEL_5df6fabf0c6f494d9df385b3fede03d2"
          }
        },
        "cb365c36f8e24030b5ae24c7380579d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d7e1061eb22474fac854c83a9338c31",
            "placeholder": "​",
            "style": "IPY_MODEL_cf2d0782229b4a289e388d1c387c7261",
            "value": "100%"
          }
        },
        "b097b3b933b04da2b2a1adf0aeb09ac6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5dd2211f3b49472589ef5ad44e9e1e77",
            "max": 600,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ad0538ea452348a1860fc9f1fea00a2e",
            "value": 600
          }
        },
        "894bb09db33d471cb270ccd2487ce427": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18da2df5d4104b82a6aa4536e63ca3f0",
            "placeholder": "​",
            "style": "IPY_MODEL_b346fdd4c6df4df0a107c8fe32ba1ed4",
            "value": " 600/600 [04:15&lt;00:00,  2.65it/s]"
          }
        },
        "5df6fabf0c6f494d9df385b3fede03d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d7e1061eb22474fac854c83a9338c31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf2d0782229b4a289e388d1c387c7261": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5dd2211f3b49472589ef5ad44e9e1e77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad0538ea452348a1860fc9f1fea00a2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "18da2df5d4104b82a6aa4536e63ca3f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b346fdd4c6df4df0a107c8fe32ba1ed4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jj132535/DeblurGAN-for-Video-Sharpness/blob/main/dncnn_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Dataset from Video File\n",
        "import cv2\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "import tqdm.notebook as tq\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from skimage.metrics import peak_signal_noise_ratio as compare_psnr"
      ],
      "metadata": {
        "id": "lMx8CYp-fkLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths to be configured by the user\n",
        "video_path = \"/content/16-9_CHUU_Strawberry_short.mp4\"  # Path to the input video file\n",
        "frame_output_path = \"/content/frames\"  # Path to save extracted frames\n",
        "checkpoint_path = \"/content/DNCNN_models/30.tar\"  # Path to the trained model checkpoint\n",
        "result_save_path = \"/content/denoised_frames\"  # Path to save denoised frames\n",
        "output_video_path = \"/content/denoised_video.mp4\"  # Path to save the final denoised video"
      ],
      "metadata": {
        "id": "XBsH1PAXfklm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 수정된 코드\n",
        "import torch.utils.data as torch_data"
      ],
      "metadata": {
        "id": "8Gn0wuLLiBUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1YVO7UpFqig"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "os.makedirs(frame_output_path, exist_ok=True)\n",
        "\n",
        "# Extract frames from video\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "frame_count = 0\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "    frame_filename = os.path.join(frame_output_path, f\"frame_{frame_count:04d}.png\")\n",
        "    cv2.imwrite(frame_filename, frame)\n",
        "    frame_count += 1\n",
        "\n",
        "cap.release()\n",
        "\n",
        "# Noise Transform\n",
        "class NoiseTransform(object):\n",
        "  def __init__(self, size=180, mode=\"training\"):\n",
        "    super(NoiseTransform, self).__init__()\n",
        "    self.size = size\n",
        "    self.mode = mode\n",
        "\n",
        "  def gaussian_noise(self, img):\n",
        "    mean = 0\n",
        "    stddev = 25\n",
        "    noise = torch.zeros(img.size()).normal_(mean, stddev/255.)\n",
        "    return noise\n",
        "\n",
        "  def __call__(self, img):\n",
        "    if self.mode in [\"training\", \"validation\"]:\n",
        "      self.gt_transform = transforms.Compose([\n",
        "        transforms.Resize((self.size, self.size), interpolation=2),\n",
        "        transforms.ToTensor()])\n",
        "      self.noise_transform = transforms.Compose([\n",
        "        transforms.Resize((self.size, self.size), interpolation=2),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(self.gaussian_noise),\n",
        "      ])\n",
        "      return self.gt_transform(img), self.noise_transform(img)\n",
        "\n",
        "    elif self.mode == \"testing\":\n",
        "      self.gt_transform = transforms.Compose([\n",
        "        transforms.Resize((self.size, self.size), interpolation=2),\n",
        "        transforms.ToTensor()])\n",
        "      return self.gt_transform(img)\n",
        "    else:\n",
        "      return NotImplementedError\n",
        "\n",
        "class NoiseDataset(torch_data.Dataset):  # 여기에서 torch_data로 변경\n",
        "    def __init__(self, root_path, size):\n",
        "        super(NoiseDataset, self).__init__()\n",
        "        self.root_path = root_path\n",
        "        self.size = size\n",
        "        self.transforms = None\n",
        "        self.examples = [os.path.join(root_path, f) for f in os.listdir(root_path) if f.endswith(\".png\")]\n",
        "\n",
        "    def set_mode(self, mode):\n",
        "        self.mode = mode\n",
        "        self.transforms = NoiseTransform(self.size, mode)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_name = self.examples[idx]\n",
        "        image = Image.open(file_name)\n",
        "\n",
        "        if self.mode == \"testing\":\n",
        "            input_img = self.transforms(image)\n",
        "            sample = {\"img\": input_img, \"file_name\": \"image_%06d.png\" % int(os.path.basename(file_name).split('_')[1].split('.')[0])}\n",
        "        else:\n",
        "            clean, noise = self.transforms(image)\n",
        "            sample = {\"img\": clean, \"noise\": noise}\n",
        "\n",
        "        return sample\n",
        "\n",
        "# Simplified DNCNN network\n",
        "class DNCNN(nn.Module):\n",
        "  def __init__(self, in_planes=3, blocks=17, hidden=64, kernel_size=3, padding=1, bias=False):\n",
        "    super(DNCNN, self).__init__()\n",
        "    self.conv_f = nn.Conv2d(in_channels=in_planes, out_channels=hidden, kernel_size=kernel_size, padding=padding, bias=bias)\n",
        "    self.conv_h = nn.Conv2d(in_channels=hidden, out_channels=hidden, kernel_size=kernel_size, padding=padding, bias=bias)\n",
        "    self.conv_l = nn.Conv2d(in_channels=hidden, out_channels=in_planes, kernel_size=kernel_size, padding=padding, bias=bias)\n",
        "\n",
        "    self.bn = nn.BatchNorm2d(hidden)\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    self.hidden_layer = self.mk_hidden_layer(blocks)\n",
        "\n",
        "  def mk_hidden_layer(self, blocks=17):\n",
        "    layers = []\n",
        "    for _ in range(blocks-2):\n",
        "      layers.append(self.conv_h)\n",
        "      layers.append(self.bn)\n",
        "      layers.append(self.relu)\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.conv_f(x)\n",
        "    out = self.relu(out)\n",
        "    out = self.hidden_layer(out)\n",
        "    out = self.conv_l(out)\n",
        "    return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing Phase\n",
        "\n",
        "os.makedirs(result_save_path, exist_ok=True)\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "test_dataset = NoiseDataset(frame_output_path, 128)\n",
        "test_dataset.set_mode(\"testing\")\n",
        "\n",
        "test_dataloader = torch_data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "net = DNCNN()\n",
        "\n",
        "if use_cuda:\n",
        "  net.to('cuda')\n",
        "else:\n",
        "  print('CUDA not available, using CPU instead')\n",
        "\n",
        "if not os.path.exists(checkpoint_path):\n",
        "    # Train the model if checkpoint does not exist\n",
        "    print('Checkpoint not found. Training the model...')\n",
        "    train_dataset = NoiseDataset(frame_output_path, 128)\n",
        "    train_dataset.set_mode(\"training\")\n",
        "    train_dataloader = torch_data.DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=0.0001, eps=1e-08)\n",
        "\n",
        "    if use_cuda:\n",
        "        net.to('cuda')\n",
        "        criterion.to('cuda')\n",
        "\n",
        "    for epoch in range(5):  # Example epoch count\n",
        "        net.train()\n",
        "        for data in train_dataloader:\n",
        "            if use_cuda:\n",
        "                clean, noise = data[\"img\"].to('cuda'), data[\"noise\"].to('cuda')\n",
        "            else:\n",
        "                clean, noise = data[\"img\"], data[\"noise\"]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            noisy_img = clean + noise\n",
        "            pred_noise = net(noisy_img)\n",
        "            loss = criterion(pred_noise, noise)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qgz5WxDHkGe6",
        "outputId": "876721d1-c9e3-4e13-ec7a-e60016e7e9eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA not available, using CPU instead\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "torch.save(net.state_dict(), checkpoint_path)\n",
        "print(f\"Model saved at {checkpoint_path}\")\n",
        "\n",
        "net.load_state_dict(torch.load(checkpoint_path, map_location='cuda' if use_cuda else 'cpu'))\n",
        "model = nn.DataParallel(net)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# Process frames and save denoised frames\n",
        "for i, data in enumerate(tq.tqdm(test_dataloader)):\n",
        "    if use_cuda:\n",
        "        img = data[\"img\"].to('cuda')\n",
        "    else:\n",
        "        img = data[\"img\"]\n",
        "\n",
        "    file_name = data[\"file_name\"]\n",
        "\n",
        "    # file_name이 리스트일 경우 첫 번째 요소만 사용\n",
        "    if isinstance(file_name, list):\n",
        "        file_name = file_name[0]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out_test = torch.clamp(img - model(img), 0., 1.)\n",
        "\n",
        "    out_img = transforms.ToPILImage()(out_test.squeeze().cpu())\n",
        "    out_img.save(os.path.join(result_save_path, file_name))\n",
        "\n",
        "\n",
        "# Combine denoised frames into video\n",
        "frame_files = sorted([f for f in os.listdir(result_save_path) if f.endswith(\".png\")])\n",
        "first_frame = cv2.imread(os.path.join(result_save_path, frame_files[0]))\n",
        "height, width, layers = first_frame.shape\n",
        "\n",
        "out = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'mp4v'), 60, (width, height))\n",
        "\n",
        "for frame_file in frame_files:\n",
        "    frame = cv2.imread(os.path.join(result_save_path, frame_file))\n",
        "    out.write(frame)\n",
        "\n",
        "out.release()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124,
          "referenced_widgets": [
            "f4ab075bc8ee4743aa81373c391ad414",
            "cb365c36f8e24030b5ae24c7380579d6",
            "b097b3b933b04da2b2a1adf0aeb09ac6",
            "894bb09db33d471cb270ccd2487ce427",
            "5df6fabf0c6f494d9df385b3fede03d2",
            "2d7e1061eb22474fac854c83a9338c31",
            "cf2d0782229b4a289e388d1c387c7261",
            "5dd2211f3b49472589ef5ad44e9e1e77",
            "ad0538ea452348a1860fc9f1fea00a2e",
            "18da2df5d4104b82a6aa4536e63ca3f0",
            "b346fdd4c6df4df0a107c8fe32ba1ed4"
          ]
        },
        "id": "80YtFjLZe1Jp",
        "outputId": "ae4f572a-77a4-4eba-c902-046dc4fde734"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved at /content/DNCNN_models/30.tar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-473e18ce9d02>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  net.load_state_dict(torch.load(checkpoint_path, map_location='cuda' if use_cuda else 'cpu'))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/600 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f4ab075bc8ee4743aa81373c391ad414"
            }
          },
          "metadata": {}
        }
      ]
    }
  ]
}